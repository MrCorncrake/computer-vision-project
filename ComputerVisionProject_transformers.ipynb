{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "lIza64i5RRx6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-06 09:44:45.082072: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-06-06 09:44:45.388996: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-06 09:44:46.699025: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-06 09:44:46.702336: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-06 09:44:52.669359: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU name:  []\n",
      "Num GPUs Available:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-06 09:45:10.998861: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-06-06 09:45:11.000157: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "print('GPU name: ', tf.config.experimental.list_physical_devices('GPU'))\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Ai4jLWdqTPe_"
   },
   "outputs": [],
   "source": [
    "train_image_path = './data/plant-pathology-2021-fgvc8/train_images'\n",
    "test_image_path = './data/plant-pathology-2021-fgvc8/test_images'\n",
    "train_df_path = './data/plant-pathology-2021-fgvc8/train.csv'\n",
    "test_df_path = './data/plant-pathology-2021-fgvc8/sample_submission.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "VljhS0gPTSeu"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(train_df_path)\n",
    "df_test=pd.read_csv(test_df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 12\n",
    "HEIGHT = 128\n",
    "WIDTH = 128\n",
    "SEED = 45\n",
    "BATCH_SIZE = 64\n",
    "input_shape = (HEIGHT, WIDTH, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14906 non-validated image filenames belonging to 12 classes.\n",
      "Found 3726 non-validated image filenames belonging to 12 classes.\n",
      "Found 3 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1/255.,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split = 0.2,\n",
    "    zoom_range = 0.2,\n",
    "    shear_range = 0.2,\n",
    "    vertical_flip = False)\n",
    "\n",
    "train_dataset = train_datagen.flow_from_dataframe(\n",
    "    df_train,\n",
    "    directory = train_image_path,\n",
    "    x_col = \"image\",\n",
    "    y_col = \"labels\",\n",
    "    target_size = (HEIGHT,WIDTH),\n",
    "    class_mode='categorical',\n",
    "    batch_size = BATCH_SIZE,\n",
    "    subset = \"training\",\n",
    "    shuffle = True,\n",
    "    seed = SEED,\n",
    "    validate_filenames = False\n",
    ")\n",
    "\n",
    "validation_dataset = train_datagen.flow_from_dataframe(\n",
    "    df_train,\n",
    "    directory = train_image_path,\n",
    "    x_col = \"image\",\n",
    "    y_col = \"labels\",\n",
    "    target_size = (HEIGHT,WIDTH),\n",
    "    class_mode='categorical',\n",
    "    batch_size = BATCH_SIZE,\n",
    "    subset = \"validation\",\n",
    "    shuffle = True,\n",
    "    seed = SEED,\n",
    "    validate_filenames = False\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rescale = 1./255\n",
    ")\n",
    "INPUT_SIZE = (HEIGHT,WIDTH,3)\n",
    "test_dataset=test_datagen.flow_from_dataframe(\n",
    "    df_test,\n",
    "    directory=test_image_path,\n",
    "    x_col='image',\n",
    "    y_col=None,\n",
    "    class_mode=None,\n",
    "    target_size=INPUT_SIZE[:2]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 6  # Size of the patches to be extract from the input images\n",
    "num_patches = (HEIGHT // patch_size) ** 2\n",
    "projection_dim = 64\n",
    "num_heads = 4\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]  # Size of the transformer layers\n",
    "transformer_layers = 8\n",
    "mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super().__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vit_classifier():\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Create patches.\n",
    "    patches = Patches(patch_size)(inputs)\n",
    "    # Encode patches.\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "        # Skip connection 2.\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Create a [batch_size, projection_dim] tensor.\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = layers.Flatten()(representation)\n",
    "    representation = layers.Dropout(0.5)(representation)\n",
    "    # Add MLP.\n",
    "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
    "    # Classify outputs.\n",
    "    logits = layers.Dense(num_classes)(features)\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=logits)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-06 09:45:15.176193: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/mnt/c/Users/Maciej/Desktop/Programowanie/computer-vision-project/venv/lib/python3.10/site-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/mnt/c/Users/Maciej/Desktop/Programowanie/computer-vision-project/venv/lib/python3.10/site-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/mnt/c/Users/Maciej/Desktop/Programowanie/computer-vision-project/venv/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/mnt/c/Users/Maciej/Desktop/Programowanie/computer-vision-project/venv/lib/python3.10/site-packages/keras/engine/training.py\", line 1050, in train_step\n        y_pred = self(x, training=True)\n    File \"/mnt/c/Users/Maciej/Desktop/Programowanie/computer-vision-project/venv/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_filevi_fd6eb.py\", line 13, in tf__call\n        patches = ag__.converted_call(ag__.ld(tf).reshape, (ag__.ld(patches), [ag__.ld(batch_size), -1, ag__.ld(patch_dims)]), None, fscope)\n\n    ValueError: Exception encountered when calling layer 'patches' (type Patches).\n    \n    in user code:\n    \n        File \"/tmp/ipykernel_1172/3102573688.py\", line 16, in call  *\n            patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n    \n        ValueError: Tried to convert 'shape' to a tensor and failed. Error: None values not supported.\n    \n    \n    Call arguments received by layer 'patches' (type Patches):\n      • images=tf.Tensor(shape=(None, None, None, None), dtype=float32)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 44\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[39mreturn\u001b[39;00m history\n\u001b[1;32m     43\u001b[0m vit_classifier \u001b[39m=\u001b[39m create_vit_classifier()\n\u001b[0;32m---> 44\u001b[0m history \u001b[39m=\u001b[39m run_experiment(vit_classifier)\n",
      "Cell \u001b[0;32mIn[12], line 26\u001b[0m, in \u001b[0;36mrun_experiment\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     18\u001b[0m checkpoint_filepath \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtmp/checkpoint\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     19\u001b[0m checkpoint_callback \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mModelCheckpoint(\n\u001b[1;32m     20\u001b[0m     checkpoint_filepath,\n\u001b[1;32m     21\u001b[0m     monitor\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mval_accuracy\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     22\u001b[0m     save_best_only\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     23\u001b[0m     save_weights_only\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     24\u001b[0m )\n\u001b[0;32m---> 26\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit_generator(\n\u001b[1;32m     27\u001b[0m     train_dataset,\n\u001b[1;32m     28\u001b[0m     validation_data\u001b[39m=\u001b[39;49mvalidation_dataset,\n\u001b[1;32m     29\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m,\n\u001b[1;32m     30\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49mtrain_dataset\u001b[39m.\u001b[39;49msamples\u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49m\u001b[39m128\u001b[39;49m,\n\u001b[1;32m     31\u001b[0m     validation_steps\u001b[39m=\u001b[39;49mvalidation_dataset\u001b[39m.\u001b[39;49msamples\u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49m\u001b[39m128\u001b[39;49m,\n\u001b[1;32m     32\u001b[0m     callbacks\u001b[39m=\u001b[39;49m[checkpoint_callback],\n\u001b[1;32m     33\u001b[0m )\n\u001b[1;32m     35\u001b[0m model\u001b[39m.\u001b[39mload_weights(checkpoint_filepath)\n\u001b[1;32m     36\u001b[0m _, accuracy, top_5_accuracy \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mevaluate(x_test, y_test)\n",
      "File \u001b[0;32m/mnt/c/Users/Maciej/Desktop/Programowanie/computer-vision-project/venv/lib/python3.10/site-packages/keras/engine/training.py:2636\u001b[0m, in \u001b[0;36mModel.fit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2624\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Fits the model on data yielded batch-by-batch by a Python generator.\u001b[39;00m\n\u001b[1;32m   2625\u001b[0m \n\u001b[1;32m   2626\u001b[0m \u001b[39mDEPRECATED:\u001b[39;00m\n\u001b[1;32m   2627\u001b[0m \u001b[39m  `Model.fit` now supports generators, so there is no longer any need to\u001b[39;00m\n\u001b[1;32m   2628\u001b[0m \u001b[39m  use this endpoint.\u001b[39;00m\n\u001b[1;32m   2629\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2630\u001b[0m warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   2631\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m`Model.fit_generator` is deprecated and \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2632\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mwill be removed in a future version. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2633\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mPlease use `Model.fit`, which supports generators.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   2634\u001b[0m     stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m   2635\u001b[0m )\n\u001b[0;32m-> 2636\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m   2637\u001b[0m     generator,\n\u001b[1;32m   2638\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[1;32m   2639\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[1;32m   2640\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   2641\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m   2642\u001b[0m     validation_data\u001b[39m=\u001b[39;49mvalidation_data,\n\u001b[1;32m   2643\u001b[0m     validation_steps\u001b[39m=\u001b[39;49mvalidation_steps,\n\u001b[1;32m   2644\u001b[0m     validation_freq\u001b[39m=\u001b[39;49mvalidation_freq,\n\u001b[1;32m   2645\u001b[0m     class_weight\u001b[39m=\u001b[39;49mclass_weight,\n\u001b[1;32m   2646\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[1;32m   2647\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[1;32m   2648\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[1;32m   2649\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[1;32m   2650\u001b[0m     initial_epoch\u001b[39m=\u001b[39;49minitial_epoch,\n\u001b[1;32m   2651\u001b[0m )\n",
      "File \u001b[0;32m/mnt/c/Users/Maciej/Desktop/Programowanie/computer-vision-project/venv/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file52cqhl30.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filevi_fd6eb.py:13\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, images)\u001b[0m\n\u001b[1;32m     11\u001b[0m patches \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mimage\u001b[39m.\u001b[39mextract_patches, (), \u001b[39mdict\u001b[39m(images\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(images), sizes\u001b[39m=\u001b[39m[\u001b[39m1\u001b[39m, ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mpatch_size, ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mpatch_size, \u001b[39m1\u001b[39m], strides\u001b[39m=\u001b[39m[\u001b[39m1\u001b[39m, ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mpatch_size, ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mpatch_size, \u001b[39m1\u001b[39m], rates\u001b[39m=\u001b[39m[\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m], padding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mVALID\u001b[39m\u001b[39m'\u001b[39m), fscope)\n\u001b[1;32m     12\u001b[0m patch_dims \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mld(patches)\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m---> 13\u001b[0m patches \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(tf)\u001b[39m.\u001b[39;49mreshape, (ag__\u001b[39m.\u001b[39;49mld(patches), [ag__\u001b[39m.\u001b[39;49mld(batch_size), \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, ag__\u001b[39m.\u001b[39;49mld(patch_dims)]), \u001b[39mNone\u001b[39;49;00m, fscope)\n\u001b[1;32m     14\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     15\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/mnt/c/Users/Maciej/Desktop/Programowanie/computer-vision-project/venv/lib/python3.10/site-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/mnt/c/Users/Maciej/Desktop/Programowanie/computer-vision-project/venv/lib/python3.10/site-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/mnt/c/Users/Maciej/Desktop/Programowanie/computer-vision-project/venv/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/mnt/c/Users/Maciej/Desktop/Programowanie/computer-vision-project/venv/lib/python3.10/site-packages/keras/engine/training.py\", line 1050, in train_step\n        y_pred = self(x, training=True)\n    File \"/mnt/c/Users/Maciej/Desktop/Programowanie/computer-vision-project/venv/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_filevi_fd6eb.py\", line 13, in tf__call\n        patches = ag__.converted_call(ag__.ld(tf).reshape, (ag__.ld(patches), [ag__.ld(batch_size), -1, ag__.ld(patch_dims)]), None, fscope)\n\n    ValueError: Exception encountered when calling layer 'patches' (type Patches).\n    \n    in user code:\n    \n        File \"/tmp/ipykernel_1172/3102573688.py\", line 16, in call  *\n            patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n    \n        ValueError: Tried to convert 'shape' to a tensor and failed. Error: None values not supported.\n    \n    \n    Call arguments received by layer 'patches' (type Patches):\n      • images=tf.Tensor(shape=(None, None, None, None), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "\n",
    "def run_experiment(model):\n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\n",
    "            keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
    "            keras.metrics.TopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    checkpoint_filepath = \"tmp/checkpoint\"\n",
    "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "        checkpoint_filepath,\n",
    "        monitor=\"val_accuracy\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "    )\n",
    "\n",
    "    history = model.fit_generator(\n",
    "        train_dataset,\n",
    "        validation_data=validation_dataset,\n",
    "        epochs=5,\n",
    "        steps_per_epoch=train_dataset.samples//128,\n",
    "        validation_steps=validation_dataset.samples//128,\n",
    "        callbacks=[checkpoint_callback],\n",
    "    )\n",
    "\n",
    "    model.load_weights(checkpoint_filepath)\n",
    "    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "vit_classifier = create_vit_classifier()\n",
    "history = run_experiment(vit_classifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "pYe7aKYzTwhm"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('complex', 0), ('frog_eye_leaf_spot', 1), ('frog_eye_leaf_spot complex', 2), ('healthy', 3), ('powdery_mildew', 4), ('powdery_mildew complex', 5), ('rust', 6), ('rust complex', 7), ('rust frog_eye_leaf_spot', 8), ('scab', 9), ('scab frog_eye_leaf_spot', 10), ('scab frog_eye_leaf_spot complex', 11)])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.class_indices.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3rD0oPTNT1o6"
   },
   "outputs": [],
   "source": [
    "preds = model.predict(test_dataset)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vVh9sMIcT4q-"
   },
   "outputs": [],
   "source": [
    "preds_disease_ind=np.argmax(preds, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "20P31eiWT6PK"
   },
   "outputs": [],
   "source": [
    "preds_disease_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "jupyter-ucz-gleb-kernel8",
   "language": "python",
   "name": "jupyter-ucz-gleb-kernel8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
